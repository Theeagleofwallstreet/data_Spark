#Set 4
#DataFrame

Ex1:
--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> val x = spark.read.load("/sampledata/Bank_full.csv")
--will fail as data is not in parquet

>>> val x = spark.read.load("/sampledata/users.parquet")
>>> x.first()

>>> val x = spark.read.format("csv").load("/sampledata/Bank_full.csv")

>>> x
res0: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 16 more fields]

>>> x.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>>val x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")

>>> x.printSchema
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

x.filter(x("marital") === "married").show(10)
x.filter(x("marital") === "married").count()

Ex2:
Option1:
option 2: directly reading json file
>>> val df = spark.read.json("/sampledata/people.json")

>>> df.show()

>>> df.printSchema()

>>> df.select("age").show()

>>> df.select("age","address").show()

>>> df.select(df("age")).show()

>>> df.select(df("age") > 30).show()

>>> df.filter(df("age") > 30).show()

>>> df.groupBy("age").count().show()

>>>df.groupBy("age").count().write.saveAsTable("dbmyfirstdf")
or
>>> df.groupBy("age").count().write.mode('overwrite').saveAsTable("dbmyfirstdf")
--to be checked

--using hive if database exists
>>>spark.sql("use ajmydb")

or
>>> val x = spark.sql("select * from dbmyfirstdf limit 10")
x = spark.sql("select * from dbmyfirstdf limit 10").show(2)

Option2:
option 1.1: defining schema and reading json file

import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val schema = new StructType().
      add("id", IntegerType, true).
      add("name", StringType, true).
      add("age", IntegerType, true).
      add("city", StringType, true).
      add("address", StringType, true)

val people = spark.read.schema(myschema).json("/sampledata/people.json")
people.show()

[--if using a multiline json file
val recipes = spark.read.option("multiline","true").json("/sampledata/recipes-orig.json")


#writes data in parquet
>>> val df = spark.read.json("/sampledata/people.json")
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")
>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

write the df in different formats
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     

>>>val df = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")
>>> df2.show(2)

>>> df.filter(df("balance") > 2000).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married")).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).count() 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).show(10)
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).show(10) 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort("count").show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort(desc("count")).show()

---to be customized for scala
>>> from pyspark.sql.functions import countDistinct
>>> df.select(countDistinct('job'))
>>> df[(df.age.isin(['35','25','45','55']))].show(10)
>>>df.groupBy(['age','y']).count()

--task(sort the above group by result in descending order of count and find top 10 counts)

>>>df.groupBy(['marital','age','y']).count()
>>>df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).show()

>>>from pyspark.sql.functions import avg,collect_list
>>>df.select(collect_list("job")).show()
>>>df.select(avg("balance"))

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... mode("overwrite").\
... csv("myfirstdf4")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).rdd.getNumPartitions()
>>> df.rdd.getNumPartitions()

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("age").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.\                                               
... write.option("header","true").\
... partitionBy("job").\
... mode("overwrite").\
... csv("myfirstdf6")

>>> newdf = spark.read.option("inferSchema","true").option("header","true").csv("myfirstdf6/job=admin.")

>>> newdf.columns

>>> df.columns

>>> newdf.show(10)

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... repartition(1).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf7")

>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.functions import UserDefinedFunction

create your function:
>>> def agegroup(x):
...     if x>50:
...             return 'old'
...     else:
...             return 'young'
... 

>>> from pyspark.sql.types import StringType
>>>udf = UserDefinedFunction(lambda x: agegroup(x), StringType())
>>>newdf = df.withColumn('age',udf(df.age))
>>> df = df.withColumn('ageGroup',udf(df.age))

-------------------------------------------
write the df with overwrite n append options
write the df in a hive table

================
#Dataset example

scala> val x = spark.read.text("/sampledata/abc1.txt").as[String]
x: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x
res1: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count
res5: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count.show()

------------
#Using reflective approach
scala> case class Person(name: String, age: Int)
defined class Person

scala> val peopleDF = sc.textFile("/sampledata/people.txt").map(_.split(",")).map(p => Person(p(0),p(1).trim.toInt))
peopleDF: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[14] at map at <console>:26

scala> val peopleDF1 = peopleDF.toDF()
peopleDF1: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> peopleDF1.createOrReplaceTempView("people")

scala> teenagersDF.map(t => "Name:" + t(0)).collect().foreach(println)

scala> teenagersDF.map(t => "Name:" + t.getAs[String]("name")).collect().foreach(println)

scala> teenagersDF.map(_.getValuesMap[Any](List("Name","age"))).collect().foreach(println)

#Using Programmatic approach
val people = sc.textFile("/sampledata/people.txt")
val schemaString = "name age"
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType, StructField, StringType}
val schema = StructType( schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
val peopleDataFrame = spark.createDataFrame(rowRDD, schema)
peopleDataFrame.createOrReplaceTempView("people")
spark.sql("select * from people").show()

---------
#schema is description of the structure of data 
val schemaUntyped = new StructType().add("name", "String").add("age", "Int")

#Describe schema of strongly typed datasets using Encoders
import org.apache.spark.sql.Encoders
case class Person(id: Long, name: String)
Encoders.product[Person].schema
