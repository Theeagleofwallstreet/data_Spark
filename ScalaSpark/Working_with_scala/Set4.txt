#Set 4
#DataFrame

#creating dataframes

scala> import spark.implicits._
import spark.implicits._

scala> val columns = Seq("ProgLang","developers")

scala> columns

scala> val data = Seq(("Go","5000"),("Python","1000"),("Scala","10000"))

scala> data

scala> val data1 = sc.parallelize(data)

scala> val df = data1.toDF()
df: org.apache.spark.sql.DataFrame = [_1: string, _2: string]

scala> df.printSchema
root
 |-- _1: string (nullable = true)
 |-- _2: string (nullable = true)


scala> val df = data1.toDF("ProgLang","developers")
df: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df.show()

#using createDataFrame
scala> val df2 = spark.createDataFrame(data1).toDF(columns:_*)
df2: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df2.show()

#using Row RDD and schema
scala> import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.types.{StringType, StructField, StructType}

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> val schema = StructType (Array (
     |                StructField("ProgLang", StringType, true),
     |                StructField("developers", StringType, true)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(ProgLang,StringType,true), StructField(developers,StringType,true))

scala> val rowRDD = data1.map(attr => Row(attr._1, attr._2))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[9] at map at <console>:30

scala> val df3 = spark.createData
createDataFrame   createDataset

scala> val df3 = spark.createDataFrame(rowRDD, schema)
df3: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df3.show()

----------------
#working with file 'After_Markings_SMPC522A1DC0040_SE-PGA454_20171012_082108_FileInfo.txt'
--refer the metadata on top 
category,startRow,nRows,type
environmentMetaData,13,2,emd
fileMetaData,15,2,fmd
systemMetaData,17,2,smd
vehicleInformationMetaData,19,2,vimd
visionMetaData,21,2,vmd
Pedestrian,23,219,Pedestrian
TwoWheeler,242,925,TwoWheeler
Unclassified3DMarking,1167,2721,Unclassified3DMarking
Vehicle,3888,2474,Vehicle

--let's create dataframe by extracting 'pedestrian' data ie 
columns from row number 23
data is 218 rows from row #24

#specifying columns
scala> val columns = "frameNumber,streamName,refId,objectType,height,direction,movement,occlusion,headOccluded,feetOccluded,overlapped,unsharp,strangePose,crossing,accessory,topLeftX,topLeftY,topRightX,topRightY,bottomRightX,bottomRightY,bottomLeftX,bottomLeftY,box3DGroundLength,box3DGroundWidth,box3DGroundCenterX,box3DGroundCenterXSigma,box3DGroundCenterY,box3DGroundCenterYSigma,box3DClosestPointX,box3DClosestPointY,box3DOrientationAngle,box3DOrientationAngleSigma,box3DHeight,box3DRelVelocityX,box3DRelVelocityXSigma,box3DRelVelocityY,box3DRelVelocityYSigma,box3DDataSource,box3DLidarInterpolationAge,box3DClassificationQuality,lidarDistanceX,lidarDistanceY,lidarVelocityX,lidarVelocityY,isInvalid,isStatic,ObjectId,Ibeo2MarkingsVersion,IdcOdExtractorVersion,clusterID,faceVisible,leftBorderVisibility,rightBorderVisibility"
scala> val columns2 = columns.split(",")
scala> val columns3 = sc.parallelize(columns2)
scala> columns3.first()
scala> val x = sc.textFile("/sampledata/After_Markings_SMPC522A1DC0040_SE-PGA454_20171012_082108_FileInfo.txt")
scala> val pedes = x.filter(line => line.contains("pedestrian"))
scala> pedes.saveAsTextFile("file:///home/hdu/outp6")
scala> val df = spark.read.option("inferSchema","true").csv("file:///home/hdu/outp6/part-00000")
scala> val df1 = df.toDF(columns.split(","): _*)
scala> df1.select("frameNumber","streamName","rightBorderVisibility").show(5)

for related examples refer:
/Bigdata_Spark/Spark_n_Scala/SparkApplicationBasicExamples/ForCluster/MySparkApps
DfourthApp.scala
EfifthApp.scala
FsixthApp.scala
GseventhApp.scala
HeighthApp.scala
-------------
#Reading and writing different files, different formats
Ex1:
--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> val x = spark.read.load("/sampledata/Bank_full.csv")
--will fail as data is not in parquet

>>> val x = spark.read.load("/sampledata/users.parquet")
>>> x.first()

>>> val x = spark.read.format("csv").load("/sampledata/Bank_full.csv")

>>> x
res0: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 16 more fields]

>>> x.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>>val x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")

>>> x.printSchema
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

x.filter(x("marital") === "married").show(10)
x.filter(x("marital") === "married").count()

Ex2:
Option1:
option 2: directly reading json file
>>> val df = spark.read.json("/sampledata/people.json")

>>> df.show()

>>> df.printSchema()

>>> df.select("age").show()

>>> df.select("age","address").show()

>>> df.select(df("age")).show()

>>> df.select(df("age") > 30).show()

>>> df.filter(df("age") > 30).show()

>>> df.groupBy("age").count().show()

>>>df.groupBy("age").count().write.saveAsTable("dbmyfirstdf")
or
>>> df.groupBy("age").count().write.mode('overwrite').saveAsTable("dbmyfirstdf")
--to be checked

--using hive if database exists
>>>spark.sql("use ajmydb")

or
>>> val x = spark.sql("select * from dbmyfirstdf limit 10")
x = spark.sql("select * from dbmyfirstdf limit 10").show(2)

Option2:
option 1.1: defining schema and reading json file

import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val schema = new StructType().
      add("id", IntegerType, true).
      add("name", StringType, true).
      add("age", IntegerType, true).
      add("city", StringType, true).
      add("address", StringType, true)

val people = spark.read.schema(myschema).json("/sampledata/people.json")
people.show()

[--if using a multiline json file
val recipes = spark.read.option("multiline","true").json("/sampledata/recipes-orig.json")


#writes data in parquet
>>> val df = spark.read.json("/sampledata/people.json")
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")
>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

write the df in different formats
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     

>>>val df = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")
>>> df2.show(2)

>>> df.filter(df("balance") > 2000).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married")).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).count() 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).show(10)
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).show(10) 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort("count").show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort(desc("count")).show()

---to be customized for scala
>>> from pyspark.sql.functions import countDistinct
>>> df.select(countDistinct('job'))
>>> df[(df.age.isin(['35','25','45','55']))].show(10)
>>>df.groupBy(['age','y']).count()

--task(sort the above group by result in descending order of count and find top 10 counts)

>>>df.groupBy(['marital','age','y']).count()
>>>df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).show()

>>>from pyspark.sql.functions import avg,collect_list
>>>df.select(collect_list("job")).show()
>>>df.select(avg("balance"))

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... mode("overwrite").\
... csv("myfirstdf4")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).rdd.getNumPartitions()
>>> df.rdd.getNumPartitions()

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("age").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.\                                               
... write.option("header","true").\
... partitionBy("job").\
... mode("overwrite").\
... csv("myfirstdf6")

>>> newdf = spark.read.option("inferSchema","true").option("header","true").csv("myfirstdf6/job=admin.")

>>> newdf.columns

>>> df.columns

>>> newdf.show(10)

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & ((col("marital") == 'married')).\
... repartition(1).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf7")

>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.functions import UserDefinedFunction

create your function:
>>> def agegroup(x):
...     if x>50:
...             return 'old'
...     else:
...             return 'young'
... 

>>> from pyspark.sql.types import StringType
>>>udf = UserDefinedFunction(lambda x: agegroup(x), StringType())
>>>newdf = df.withColumn('age',udf(df.age))
>>> df = df.withColumn('ageGroup',udf(df.age))

-------------------------------------------
write the df with overwrite n append options
write the df in a hive table

================
#Dataset example

scala> val x = spark.read.text("/sampledata/abc1.txt").as[String]
x: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x
res1: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count
res5: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count.show()

------------
#Using reflective approach
scala> case class Person(name: String, age: Int)
defined class Person

scala> val peopleDF = sc.textFile("/sampledata/people.txt").map(_.split(",")).map(p => Person(p(0),p(1).trim.toInt))
peopleDF: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[14] at map at <console>:26

scala> val peopleDF1 = peopleDF.toDF()
peopleDF1: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> peopleDF1.createOrReplaceTempView("people")

scala> teenagersDF.map(t => "Name:" + t(0)).collect().foreach(println)

scala> teenagersDF.map(t => "Name:" + t.getAs[String]("name")).collect().foreach(println)

scala> teenagersDF.map(_.getValuesMap[Any](List("Name","age"))).collect().foreach(println)

#Using Programmatic approach
val people = sc.textFile("/sampledata/people.txt")
val schemaString = "name age"
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType, StructField, StringType}
val schema = StructType( schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
val peopleDataFrame = spark.createDataFrame(rowRDD, schema)
peopleDataFrame.createOrReplaceTempView("people")
spark.sql("select * from people").show()

---------
#schema is description of the structure of data 
val schemaUntyped = new StructType().add("name", "String").add("age", "Int")

#Describe schema of strongly typed datasets using Encoders
import org.apache.spark.sql.Encoders
case class Person(id: Long, name: String)
Encoders.product[Person].schema
