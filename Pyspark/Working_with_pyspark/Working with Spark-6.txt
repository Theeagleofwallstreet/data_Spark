>>> from pyspark.sql import Row
>>> file = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> partfile = file.map(lambda n: n.split(" "))
>>> partfile.collect()
>>> filecont = partfile.map(lambda n: Row(keyword=n[0],word1=n[1],word2=n[2],word3=n[3]))
>>> schemafilecontdf = spark.createDataFrame(filecont)
>>> schemafilecontdf.show()
>>> schemafilecontdf.createOrReplaceTempView("fileindf")
>>> spark.sql("select * from fileindf")
-------------------
>>> file = spark.read.text("mydata/mydata-local1/abc1.txt")
>>> file.show(truncate=False)

-------------------
>>> file = sc.textFile("mydata/mydata-local1/people.txt")
>>> partfile = file.map(lambda n: n.split(","))
>>> filecont = partfile.map(lambda n: (n[0],n[1])
... )
>>> for i in filecont.take(10):
...     print(i)
>>> schemat = "name age"
>>> from pyspark.sql.types import *
>>> fields = [StructField(field_name, StringType(), True) for field_name in schemat.split()]
>>> schema = StructType(fields)
>>> df = spark.createDataFrame(filecont,schema)
>>> df.show()


