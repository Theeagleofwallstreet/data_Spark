pyspark
--using hive
>>>spark.sql("use ajmydb")
>>> x = spark.sql("select * from bankdata limit 10")
>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>

--using file from hdfs and using 'sc' using rdd api
>>> x = sc.textFile("mydata/mydata-local1/Bank_full.csv")
>>> type(x)
    <class 'pyspark.rdd.RDD'>
>>> x.first() [it will fail]

>>> for i in x.take(5):
...     print(i)
... 
serNo,age,job,marital,education,defaulter,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,y
1,58,management,married,tertiary,no,2143,yes,no,unknown,5,may,261,1,-1,0,unknown,no
2,44,technician,single,secondary,no,29,yes,no,unknown,5,may,151,1,-1,0,unknown,no
3,33,entrepreneur,married,secondary,no,2,yes,yes,unknown,5,may,76,1,-1,0,unknown,no
4,47,blue-collar,married,unknown,no,1506,yes,no,unknown,5,may,92,1,-1,0,unknown,no

--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> x = spark.read.load("mydata/mydata-local1/Bank_full.csv")
--will fail as data is not in parquet

>>> x = spark.read.load("mydata/mydata-local1/users.parquet")
>>> x.first()

>>> x = spark.read.format("csv").load("mydata/mydata-local1/Bank_full.csv")
>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>
>>> x.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>> x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("mydata/mydata-local1/Bank_full.csv")
>>> type(x)                                                                                                                                          
<class 'pyspark.sql.dataframe.DataFrame'>
>>> x.printSchema()root
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

x.filter(x.marital == 'married')

>>> x.filter(x.marital == 'married').count()
-------------
RDD way
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> type(x)
>>> for i in x.take(5):
...     print(i)

Dataframe way
>>> y = spark.read.format("text").load("mydata/mydata-local1/abc1.txt")
>>> type(y)
>>> y.show()

using rdd
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> x.getNumPartitions()
2
>>> y = x.map(lambda n: n.upper())
>>> y.getNumPartitions()
2
>>> z = y.map(lambda n: n.lower())
>>> z.getNumPartitions()
2
>>> a = z.map(lambda n: n.upper())
>>> a.getNumPartitions()
2
>>> b = a.map(lambda n: (n,1))
>>> b.getNumPartitions()
2
>>> c = b.map(lambda n: (n,100))
>>> c.getNumPartitions()
2
>>> c.collect()


---------
>>> x = sc.textFile("mydata/mydata-local1/Bank_full.csv",3)
>>> y = x.map(lambda n: n.upper())
>>> z = y.map(lambda n: n.lower())
>>> a = z.map(lambda n: n.upper())
>>> b = a.map(lambda n: (n,1))
>>> c = b.map(lambda n: (n,100))
>>> c.toDebugString()
>>> c.collect()

or
>>> sc.textFile("mydata/mydata-local1/Bank_full.csv",3).map(lambda n: n.upper()).map(lambda n: n.lower()).map(lambda n: n.upper()).map(lambda n: (n,1)).map(lambda n: (n,100)).first()
((u'SERNO,AGE,JOB,MARITAL,EDUCATION,DEFAULTER,BALANCE,HOUSING,LOAN,CONTACT,DAY,MONTH,DURATION,CAMPAIGN,PDAYS,PREVIOUS,POUTCOME,Y', 1), 100)  

or

>>> x = range(1,10)
>>> x
[1, 2, 3, 4, 5, 6, 7, 8, 9]

>>> y = sc.parallelize(x,2)
>>> y.getNumPartitions()
2
>>> z = y.filter(lambda n: n%2==0)
>>> a = z.map(lambda n: n*100)
>>> a.collect()
[200, 400, 600, 800] 

or

Problem:
SparkSession available as 'spark'.
>>> x = range(1,10)
>>> y = sc.parallelize(x,2)

>>> z = fill the code


>>> a = fill the code

>>> print(a.toDebugString())
(2) PythonRDD[2] at RDD at PythonRDD.scala:53 []
 |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262 []

>>> fill in the blanks...............
result---->[200, 400, 600, 800]                                                                                                                                 
>>> 










