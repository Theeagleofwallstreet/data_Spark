1. Scenario 1:
You have been given salary details of employees in a file 'salarydet.txt' in the folder 'sample-files-for-test'.

This file contains
id:employee_id
name:employee_name
bonus:bonus_percentage
Payment:yop:year_of_payment & current_salary:current_salary

Note**
--File has data delimited by delimiters
--Payment column supports 'map' datatype


sample contents of salarydet.txt
----------
10,smith,10,1styear:210000
20,peter,10,1styear:410000


Tasks to Do:
Load this data into a dir 'salary' on hdfs
Create a MANAGED hive table 'salarydata1m' which can hold this data.The table should be in default location of hive.
Load data into table using load statement from hdfs
Check if file exists in your 'salary' directory
Create another EXTERNAL table 'salarydata2m' pointing to your MANAGED table location

Query the table MANAGED/EXTERNAL table to find these:


--show current_salary as salary along with other columns where year_of_payment value is 1styear.
--show current_salary as salary along with other columns where year_of_payment value is 1styear and order by salary in desc order
  --check if there were any null values and analyze why
  --check if MapReduce job was trigger and see how much time it took
--use function to round off the value of total salary after appling bonus percentage to salary where year_of_payment is 1styear and
display this data in desc order
--find total sum paid as salary (salar+bonus) where year_of_payment is 1styear
--drop your external table

Using Pyspark,query the managed table:
--test the same queries from PySpark instead of hive
--check if it triggers any MapReduce
--save output of any query in a json file on hdfs
--save output of any query in a default format on hdfs(parquet)

--check files in hdfs to confirm if data was written
--if multiple files were written in hdfs, investigate why?
--write the code to have data written only in one file

#Optional
-- read data from the file (salarydet.txt) in HDFS location using PySpark(Dataframes/RDD)
   --This location could be your hive table folder
   --This location could be your hdfs directory
--filter the data where year_of_payment is not 1st year and save this output in a CSV file on hdfs

Scenario 2:

You have been given sentiments data in a file 'cv000_29416.txt' in the folder 'sample-files-for-test'.

Tasks to do:
Load this data in hdfs by creating a directory 'sentiments'
--write code in scala/python to perform wordcount on this file and then
    --find how many words were found by getting a count
    --show the output of this wordcount code
    --print only 10 entries from this result
    --save the output on hdfs in folder 'result' > check files on hdfs
    --save the output in one file on hdfs if multiple files were created in previous save operation

Scenario 3:
You have been given bankdata in a file 'Bank_full.csv' in the folder 'sample-files-for-test'.
--This data contains data of customers who were contacted as a part of marketing campaign to understand if they would be interested in 
--having a deposit in bank (shown by last column y)

Tasks to Do:
Using Hive:

--Load data into hdfs without creating a specific directory on hdfs, i.e. directly on hdfs '/'
--Create hive table 'bankdata' to hold this data and then load this data into table from your local filesystem ie from 'sample-files-for-test' or wherever this file was downloaded
--get count of rows in table
--Load the data into hive table without overwriting using the same file. Your table should have the data 3 times.
--get count of rows in the table
--Run queries to:
    --Get count of enteries for each education category(Hint:Group by)
    --Get count of enteries for each education category for a particlar age(Hint: Group by)
    --Get count of enteries for each age and education category
    --Get count of enteries for each age and education category and get the top 10;
    --Get count of subscriptions (column y) for each marital category and check if marital status affects decision of subscription
    --Find avg balance for each category in previous query

Using Spark>PySpark/Spark-Shell
--Load the data from 'Bank_full.csv' file on hdfs into a Spark dataframe(df)
--Write code to Run Queries to:
  --read data from dataframe and show 5 rows
  --filter data from dataframe to contain data of 'married' customers 
  --get a count of records where customers are married
  --get a count of records where customers are married and have a 'blue-collar' job
  --group data based on age and get a count
  --group data based on age and marital and get a count
  --group data based on age,marital,y and get a count
  --filter data where customers are married and below age of 35.Save this data into a different df & on hdfs as a csv file.
  --same as above but save in JSON format.

--You can also load data from 'Bank_full.csv' file on hdfs into a Spark RDD
--Write code to work on RDD:
   --load data in a RDD
   --covert all data into uppercase
   --print first two records/rows

Optional:

Perform the operations done in dataframe section but using RDD

Scenario 4:
You are given a collection, a list of words
>>> x = ["welcome","hello","hell","well","cool","cold"]
--To Write (write the missing steps of code)
--To Write (write the missing steps of code)
>>> res = z.map(lambda n : (n[0],n))
>>> for i in res.take(2):
...     print(i)
... 
('w', 'welcome')
('h', 'hello')

--Perform a group by on this collection RDD to get a list of words grouped by their first letter
--show the RDDs created for your result using 'toDebugString()'

Scenario 5:
Read data from hive in spark from your previously created 'bankdata' and filter it based on some conditions,
finally save it as a table 'newtblbank'.
--check in hive if table exists
--run a simple query to query data from hive table (from PySpark)
--check the table's location



