#Set 1
Pyspark

#RDD
--RDD way
--Reading a file from HDFS/local filesystem
##
x = sc.textFile("mydata/mydata-local1/abc1.txt")

type(x)
for i in x.take(5):
... print(i)
x.first()
x.collect()
--refer Spark UI

--Dataframe way
##
y = spark.read.format("text").load("mydata/mydata-local1/abc1.txt")
type(y)
y.show(1)
y.first()
y.collect()

--to see the number of partitions of RDD
x.getNumPartitions()

--to see lineage
##
x.toDebugString()
print(x.toDebugString())

##
x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
print(x.toDebugString())
(4) mydata/mydata-local1/abc1.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  mydata/mydata-local1/abc1.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
x.getNumPartitions()
x.saveAsTextFile("mydata/rddo1")

--try above option without partition option and then save 
x = sc.textFile("mydata/mydata-local1/abc1.txt")
x.getNumPartitions()
x.saveAsTextFile("mydata/rddo2")


--using a different file from HDFS/local filesystem
##
x = sc.textFile("mydata/mydata-local1/Bank_full.csv")
type(x)

for i in x.take(5):
...     print(i)
... 

x.first()

--changing/allocating number of partitions
##
x = sc.textFile("mydata/mydata-local1/Bank_full.csv",3)
x.getNumPartitions()
--refer Spark UI

--No Spark
##
x = range(1,10)
for i in x:
...   print(i)

or

for i in x:
...     print(i,end=" ")

--Using spark to work on collections
##
y = sc.parallelize(x,2)
or
y = sc.parallelize(range(1,10))
y.collect()
y.first()
y.count()
y.getNumPartitions()

##
x = "welcome"
y = sc.parallelize(x)
y.first()


--Using lambda for anonymous functions

Ex1:
x = "welcome"
y = sc.parallelize(x)
y.first()
z = y.map(lambda n: n.upper())
z.first()
t = z.filter(lambda n: n == 'W')
t.count()
t = z.filter(lambda n: n == 'E')
t.count()

Ex2:
x = sc.textFile("mydata/mydata-local1/cv000_29416.txt")
y = x.map(lambda n: n.upper())
y.getNumPartitions()
y.first()

z = y.map(lambda n: n.lower())
z.getNumPartitions()
z.first()

a = z.map(lambda n: n.upper())
a.getNumPartitions()
a.first()

b = a.map(lambda n: (n,1))
b.getNumPartitions()
b.first()

c = b.map(lambda n: (n,100))

c.getNumPartitions()
c.first()
c.collect()
c.toDebugString()

Ex3:
--Using lambda and chaining transformations
sc.textFile("mydata/mydata-local1/Bank_full.csv",3).map(lambda n: n.upper()).map(lambda n: n.lower()).map(lambda n: n.upper()).
map(lambda n: (n,1)).map(lambda n: (n,100)).first()

Ex4:
--Using lambda and parallelize
x = range(1,10)
x
y = sc.parallelize(x,2)
y.getNumPartitions()
z = y.filter(lambda n: n%2==0)
a = z.map(lambda n: n*100)
a.collect()


Ex5:
x = range(1,10)
y = sc.parallelize(x)
y
z = y.map(lambda n : n*100)
z
z.collect()                                                                                                   

===================================
#Set 2

x = sc.textFile("mydata/mydata-local1/abc1.txt")
y = x.flatMap(lambda n: n.split(" "))
numTs = y.filter(lambda n: 't' in n)
numWs = y.filter(lambda n: 'w' in n)
numTs.count()
numWs.count()
print("number of ts in file:" ,numTs.count(),"number of ws in file: " ,numWs.count())

------------
Application

DAG /Job 0/ Stage 0
x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
y = x.flatMap(lambda n: n.split(" "))
z = y.map(lambda word: (word,1))
z.first()

DAG Job 1/Stage 1
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.collect()

DAG  Job 2/Stage 2
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.saveAsTextFile("mydata/rddo2")

DAG Job 3/Stage 3 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.repartition(1).saveAsTextFile("mydata/rddo3")

DAG Job 4/Stage 4 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
wordcount = z.reduceByKey(lambda a,b: a+b)
wordcount.saveAsTextFile("mydata/rddo4")

DAG Job 5/Stage 5 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
#wordcount = z.reduceByKey(lambda a,b: a+b)
wordcount.repartition(1).saveAsTextFile("mydata/rddo5")

--Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content

--When working with directory
>>>mydata = sc.wholeTextFiles("mydata/mydata-local1")
>>> for i in mydata.keys().take(10):
...     print(i)

--caching
x = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).cache()
x.collect()

--persisting
y = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).persist()
y.collect()

Resource mgr > application > applicationMaster

--Persisting Options
StorageLevel.
StorageLevel.MEMORY_ONLY_SER ---SAME AS memory_only , stores RDD as serialized objects in JVM memory
StorageLevel.MEMORY_ONLY_2 ---replicated cached partitions of RDD
StorageLevel.MEMORY_ONLY_SER_2 
StorageLevel.MEMORY_AND_DISK
StorageLevel.MEMORY_ONLY
StorageLevel.MEMORY_AND_DISK_2
StorageLevel.MEMORY_AND_DISK_ONLY
StorageLevel.DISK_ONLY
StorageLevel.DISK_ONLY_2

>>> from pyspark import StorageLevel
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>> y = x.flatMap(lambda n: n.split(" "))
>>> z = y.map(lambda word: (word,1))

>>> z.persist(StorageLevel.DISK_ONLY)
result is 'PythonRDD - Disk serialized 1x Replicated'

>>> z.unpersist()

>>> z.persist(StorageLevel.MEMORY_ONLY)
result is 'PythonRDD - Memory serialized 1x Replicated'

>>> z.persist(StorageLevel.MEMORY_AND_DISK_ONLY)

>>> z.collect()

>>> print(z.getStorageLevel())

===================================
#Set 3

Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content.
-groupBy
-reduceBY
-groupByKey
-reduceByKey
-countByKey
-zip/sub/union/join/...


Ex1:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
y = x.map(lambda n: (n.split(" "))).map(lambda n: (n[0],n))
grouped = y.groupByKey().mapValues(list)
grouped.collect()
count = y.groupByKey().count()

Ex2:
option1:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
pairrdd.first()

for i in pairrdd.keys().take(10):
...     print(i)

for i in pairrdd.values().take(10):
...     print(i)

y = sc.textFile("mydata/mydata-local1/abc2.txt")
pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))

for i in pairrdd.take(20):
...     print(i)
for i in pairrdd2.take(20):
...     print(i)


for i in pairrdd.sortByKey().take(20):
...     print(i)
for i in pairrdd2.sortByKey().take(20):
...     print(i)

option2:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
result = x.map(lambda n: n.split(" ")).sortBy(lambda n: n[0])
for i in pairrdd.sortBy(lambda n: n[0]).take(20):
...     print(i)

option3:
for i in pairrdd.sortByKey(True, 1).take(5):
...     print(i)

result = pairrdd.sortByKey(ascending=True).values()
for i in result.take(20):
...     print(i)

result1 = pairrdd.sortByKey(ascending=False).values()
for i in result1.take(20):
...     print(i)

pairrdd.countByKey().items()

reduced = pairrdd.reduceByKey(lambda a,b: a+b)
for i in reduced.take(10):
...     print(i)

Ex3:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
y = sc.textFile("mydata/mydata-local1/abc2.txt")

pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
union_data = pairrdd.union(pairrdd2)
union_data.collect()
for i in union_data.take(50):
...     print(i)
union_data.getNumPartitions()
pairrdd.getNumPartitions()
pairrdd2.getNumPartitions()
pairrdd.subtract(pairrdd2)
pairrdd.intersection(pairrdd2)
pairrdd.zip(pairrdd2)
pairrdd.join(pairrdd2)


--Building Applications(examples)
PyCharm:

testapp1.py

from pyspark import SparkConf, SparkContext
sc = SparkContext(master="local",appName="testAppl")
mydata = sc.textFile("file:///D:\\GitContent\\Datasets\\Datasets\\cv000_29416.txt",2)
step1 = mydata.map(lambda n: n.upper())
print(step1.collect())
print("number of partitions:", mydata.getNumPartitions())

--Run application from command line
spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp1.py

testapp2.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("sparkdfex").getOrCreate()
x = spark.read.format("json").option("inferSchema","true").option("header","true").\
        load("file:///D:\\GitContent\\Datasets\\Datasets\\employees.json")
x.show(10)
x.printSchema()

--Run application from command line
spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp2.py

=====================================
#Set 4
#DataFrame

Ex1:
--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> x = spark.read.load("mydata/mydata-local1/Bank_full.csv")
--will fail as data is not in parquet

>>> x = spark.read.load("mydata/mydata-local1/users.parquet")
>>> x.first()

>>> x = spark.read.format("csv").load("mydata/mydata-local1/Bank_full.csv")

>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>

>>> x.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>> x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("mydata/mydata-local1/Bank_full.csv")

>>> type(x)                                                                                                                                          
<class 'pyspark.sql.dataframe.DataFrame'>

>>> x.printSchema()root
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

>>>x.filter(x.marital == 'married')

>>>x.filter(x.marital == 'married').count()

Ex2:
Option1:
option 2: directly reading json file
>>> df = spark.read.json("mydata/mydata-local1/people.json")

>>> df.show()

>>> df.printSchema()

>>> df.select("age")

>>> df.select("age","address")

>>> df.select(df['age'])

>>> df.select(df['age'] > 30)

>>> df.filter(df['age'] > 30)

>>> df.groupBy("age").count()

--if database ajmydb exists
>>>df.groupBy("age").count().write.saveAsTable("ajmydb.dbmyfirstdf")
or
>>> df.groupBy("age").count().write.mode("overwrite").saveAsTable("ajmydb.dbmyfirstdf")

--if not
df.groupBy("age").count().write.saveAsTable("dbmyfirstdf")

--using hive if database exists
>>>spark.sql("use ajmydb")

or
>>> x = spark.sql("select * from dbmyfirstdf limit 10")
>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>
x = spark.sql("select * from dbmyfirstdf limit 10").show(2)

Option2:
option 1.1: defining schema and reading json file

from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
people.show()

[--if using a multiline json file
"""recipes = spark.read.option("multiLine","true").json("mydata/mydata-local1/recipes2.json")"""]

Option 1.2
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").config("spark.sql.warehouse.dir","/user/win10/spark-warehouse").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
people.show()
people.write.saveAsTable("peopledf1")


--if using Hive/if database exists
>>>spark.sql("use ajmydb")

--if not then directly
>>>spark.sql("show tables")
>>>spark.sql("select * from dbmyfirstdf").show()

#writes data in parquet
>>> df = spark.read.json("mydata/mydata-local1/people.json")
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")
>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

Ex3:
>>> df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("mydata/mydata-local1/Bank_full.csv")
>>> df.show(5)
>>> df.columns
>>> df[df.age > 80].show()
>>> df[df.age > 80].count()
>>>df.summary().show()

write the df in different formats
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     
>>> df2.show(2)

>>> df.filter(df['balance'] > 2000).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married')).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).show(10)
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).sort('balance',ascending=False).show(10)
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False).show(5)
>>> selecteddf = df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False)
>>> df.count()
>>> selecteddf.count()
>>>df.groupBy(df.job).count()

>>> from pyspark.sql.functions import countDistinct
>>> df.select(countDistinct('job'))
>>> df[(df.age.isin(['35','25','45','55']))].show(10)
>>>df.groupBy(['age','y']).count()

--task(sort the above group by result in descending order of count and find top 10 counts)

>>>df.groupBy(['marital','age','y']).count()
>>>df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).show()

>>>from pyspark.sql.functions import avg,collect_list
>>>df.select(collect_list("job")).show()
>>>df.select(avg("balance"))

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... mode("overwrite").\
... csv("myfirstdf4")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).rdd.getNumPartitions()
>>> df.rdd.getNumPartitions()

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... partitionBy("age").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.\                                               
... write.option("header","true").\
... partitionBy("job").\
... mode("overwrite").\
... csv("myfirstdf6")

>>> newdf = spark.read.option("inferSchema","true").option("header","true").csv("myfirstdf6/job=admin.")

>>> newdf.columns

>>> df.columns

>>> newdf.show(10)

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... repartition(1).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf7")

>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.functions import UserDefinedFunction

create your function:
>>> def agegroup(x):
...     if x>50:
...             return 'old'
...     else:
...             return 'young'
... 

>>> from pyspark.sql.types import StringType
>>>udf = UserDefinedFunction(lambda x: agegroup(x), StringType())
>>>newdf = df.withColumn('age',udf(df.age))
>>> df = df.withColumn('ageGroup',udf(df.age))

-------------------------------------------
write the df with overwrite n append options
write the df in a hive table

================
#Set 5
x = spark.read.format("csv").option("header","true").option("inferSchema","true").load("I:\\GitContent\\Datasets\\Datasets\\Bank_full.csv")
x.columns
df[df.age > 80]
df.createOrReplaceTempView("test")
spark.sql("select * from test where age > 80" ).show()
df[df.age > 80].count()
df.summary().show()
df[(df.age > 80) & (df.marital=='married')],show()
df[(df.age > 80) & (df.marital=='married')].sort('balance',ascending=False).show(5)

df[(df.age.isin(['82','15','17','35']))].show(5)

df.groupby(['age','y']).count()

#If we want to use different fields for sorting, or DESC instead of ASC
# we want to sort by our calculated field (count), this field needs to become part of 
# the DataFrame. After grouping , we get back a different type, called 
# a GroupByObject. 
# So we need to convert it back to a DataFrame. 
x = df.groupby(['marital', 'y']).count()
x.sort('marital','count',ascending=False).show()

#additionally filter grouped data using a HAVING condition. In Pandas, 
# you can use .filter() and provide a Python function (or a lambda) 
# that will return True if the group should be included into the result.

y = df[df.marital == 'married'].groupby('y').count()
y.show()
from pyspark.sql.functions import col
y.filter(col("count")>3000).show()

#creating new df
df1 = df.groupby(['age','y']).count()

>>> from pyspark.sql import Row
>>> file = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> partfile = file.map(lambda n: n.split(" "))
>>> partfile.collect()
>>> filecont = partfile.map(lambda n: Row(keyword=n[0],word1=n[1],word2=n[2],word3=n[3]))
>>> schemafilecontdf = spark.createDataFrame(filecont)
>>> schemafilecontdf.show()
>>> schemafilecontdf.createOrReplaceTempView("fileindf")
>>> spark.sql("select * from fileindf")
-------------------
>>> file = spark.read.text("mydata/mydata-local1/abc1.txt")
>>> file.show(truncate=False)

-------------------
>>> file = sc.textFile("mydata/mydata-local1/people.txt")
>>> partfile = file.map(lambda n: n.split(","))
>>> filecont = partfile.map(lambda n: (n[0],n[1])
... )
>>> for i in filecont.take(10):
...     print(i)
>>> schemat = "name age"
>>> from pyspark.sql.types import *
>>> fields = [StructField(field_name, StringType(), True) for field_name in schemat.split()]
>>> schema = StructType(fields)
>>> df = spark.createDataFrame(filecont,schema)
>>> df.show()
======================
