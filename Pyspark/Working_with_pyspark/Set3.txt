#Set 3

Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content.
-groupBy
-reduceBY
-groupByKey
-reduceByKey
-countByKey
-zip/sub/union/join/...


Ex1:
x = sc.textFile("/sampledata/abc1.txt")
y = x.map(lambda n: (n.split(" "))).map(lambda n: (n[0],n))
grouped = y.groupByKey().mapValues(list)
grouped.collect()
count = y.groupByKey().count()

Ex2:
option1:
x = sc.textFile("/sampledata/abc1.txt")
pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
pairrdd.first()

for i in pairrdd.keys().take(10):
...     print(i)

for i in pairrdd.values().take(10):
...     print(i)

y = sc.textFile("/sampledata/abc2.txt")
pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))

for i in pairrdd.take(20):
...     print(i)
for i in pairrdd2.take(20):
...     print(i)


for i in pairrdd.sortByKey().take(20):
...     print(i)
for i in pairrdd2.sortByKey().take(20):
...     print(i)

option2:
x = sc.textFile("/sampledata/abc1.txt")
result = x.map(lambda n: n.split(" ")).sortBy(lambda n: n[0])
for i in pairrdd.sortBy(lambda n: n[0]).take(20):
...     print(i)

option3:
for i in pairrdd.sortByKey(True, 1).take(5):
...     print(i)

result = pairrdd.sortByKey(ascending=True).values()
for i in result.take(20):
...     print(i)

result1 = pairrdd.sortByKey(ascending=False).values()
for i in result1.take(20):
...     print(i)

pairrdd.countByKey().items()

reduced = pairrdd.reduceByKey(lambda a,b: a+b)
for i in reduced.take(10):
...     print(i)

Ex3:
x = sc.textFile("/sampledata/abc1.txt")
y = sc.textFile("/sampledata/abc2.txt")

pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
union_data = pairrdd.union(pairrdd2)
union_data.collect()
for i in union_data.take(50):
...     print(i)
union_data.getNumPartitions()
pairrdd.getNumPartitions()
pairrdd2.getNumPartitions()
pairrdd.subtract(pairrdd2)
pairrdd.intersection(pairrdd2)
pairrdd.zip(pairrdd2)
pairrdd.join(pairrdd2)


#Additional way to create pairRDD
>>> x = sc.textFile("/sampledata/abc1.txt")
>>> paira = x.map(lambda n: (n.split(" ")[0],n))
>>> paira.collect
<bound method RDD.collect of PythonRDD[2] at RDD at PythonRDD.scala:53>
>>> paira.collect()
[('This', 'This is my sample file'), ('which', 'which I want to test'), ('This', 'This is a new file')]
>>> y = sc.textFile("/sampledata/abc2.txt")
>>> pairb = y.map(lambda n: (n.split(" ")[0],n))
>>> pairb.collect()
[('This', 'This content is new'), ('I', 'I want to use'), ('which', 'which I want to test'), ('This', 'This is a new file'), ('This', 'This and that is same')]
>>> paira.intersection(pairb).collect()
[('which', 'which I want to test'), ('This', 'This is a new file')]
>>> pairb.subtract(paira).collect()
[('This', 'This and that is same'), ('This', 'This content is new'), ('I', 'I want to use')]

--Building Applications(examples)
PyCharm:

testapp1.py

from pyspark import SparkConf, SparkContext
sc = SparkContext(master="local",appName="testAppl")
mydata = sc.textFile("file:///D:\\GitContent\\Datasets\\Datasets\\cv000_29416.txt",2)
step1 = mydata.map(lambda n: n.upper())
print(step1.collect())
print("number of partitions:", mydata.getNumPartitions())

--Run application from command line
spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp1.py

testapp2.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("sparkdfex").getOrCreate()
x = spark.read.format("json").option("inferSchema","true").option("header","true").\
        load("file:///D:\\GitContent\\Datasets\\Datasets\\employees.json")
x.show(10)
x.printSchema()

--Run application from command line
spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp2.py

=====================================
