>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>> print(x.toDebugString())
(4) mydata/mydata-local1/abc1.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  mydata/mydata-local1/abc1.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
>>> x.getNumPartitions()
4
>>> x.saveAsTextFile("mydata/rddo1")
>>> quit()

------------
Main Application
DAG /Job 0/ Stage 0
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>> y = x.flatMap(lambda n: n.split(" "))
>>> z = y.map(lambda word: (word,1))
	>>> z.first()

Job 1/Stage 1
DAG
>>># x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>># y = x.flatMap(lambda n: n.split(" "))
>>># z = y.map(lambda word: (word,1))
	>>> z.collect()

Job 2/Stage 2
DAG 
>>># x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>># y = x.flatMap(lambda n: n.split(" "))
>>># z = y.map(lambda word: (word,1))
	>>> z.saveAsTextFile("mydata/rddo2")

Job 3/Stage 3 ( involves shuffling)
>>># x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>># y = x.flatMap(lambda n: n.split(" "))
>>># z = y.map(lambda word: (word,1))
	>>>z.repartition(1).saveAsTextFile("mydata/rddo3")

Job 4/Stage 4 ( involves shuffling)
>>># x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>># y = x.flatMap(lambda n: n.split(" "))
>>># z = y.map(lambda word: (word,1))
	>>> wordcount = z.reduceByKey(lambda a,b: a+b)
	>>> wordcount.saveAsTextFile("mydata/rddo4")

Job 5/Stage 5 ( involves shuffling)
>>># x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>># y = x.flatMap(lambda n: n.split(" "))
>>># z = y.map(lambda word: (word,1))
>>>#wordcount = z.reduceByKey(lambda a,b: a+b)
	>>>wordcount.repartition(1).saveAsTextFile("mydata/rddo5")

>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).cache()
>>> x.collect()

>>> y = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).persist()
>>> y.collect()

Resource mgr >pplication > applicationMaster

>>> 
>>> from pyspark import StorageLevel
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>> y = x.flatMap(lambda n: n.split(" "))
>>> z = y.map(lambda word: (word,1))
>>> z.persist(StorageLevel.DISK_ONLY)
result is 'PythonRDD - Disk serialized 1x Replicated'

>>> z.unpersist()
PythonRDD[2] at RDD at PythonRDD.scala:53
>>> z.persist(StorageLevel.MEMORY_ONLY)
PythonRDD[2] at RDD at PythonRDD.scala:53


>>> z.persist(StorageLevel.MEMORY_AND_DISK_ONLY)
>>> z.collect()
result is 'PythonRDD - Memory serialized 1x Replicated'

result is 'PythonRDD - Disk serialized 1x Replicated'

StorageLevel.MEMORY_ONLY_SER ---SAME AS memory_only , stores RDD as serialized objects in JVM memory
StorageLevel.MEMORY_ONLY_2 ---replicated cached partitions of RDD
StorageLevel.MEMORY_ONLY_SER_2 
StorageLevel.MEMORY_AND_DISK
StorageLevel.MEMORY_AND_DIS
StorageLevel.MEMORY_AND_DISK_2
StorageLevel.DISK_ONLY
StorageLevel.DISK_ONLY_2

>>> print(z.getStorageLevel())
Disk Memory Serialized 1x Replicated
-----------------------------------------
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> y = x.flatMap(lambda n: n.split(" "))
>>> z = y.filter(lambda n: 't' in n)
>>> numWs = y.filter(lambda n: 'w' in n)
>>> numTs.count()
16
>>> numWs.count()
12

>>> print("number of ts in file:" ,numTs.count(),"number of ws in file: " ,numWs.count())

----------------
>>>mydata = sc.wholeTextFiles("mydata/mydata-local1")
>>> for i in mydata.keys().take(10):
...     print(i)

>>> type(mydata)
<class 'pyspark.rdd.RDD'>

----------
Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content
groupBy
reduceBY
groupByKey
reduceByKey
countByKey
zip/sub/union/join/...
------------


















