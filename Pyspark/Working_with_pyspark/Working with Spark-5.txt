x = spark.read.format("csv").option("header","true").option("inferSchema","true").load("I:\\GitContent\\Datasets\\Datasets\\Bank_full.csv")
x.columns
df[df.age > 80]
df.createOrReplaceTempView("test")
spark.sql("select * from test where age > 80" ).show()
df[df.age > 80].count()
df.summary().show()
df[(df.age > 80) & (df.marital=='married')],show()
df[(df.age > 80) & (df.marital=='married')].sort('balance',ascending=False).show(5)

df[(df.age.isin(['82','15','17','35']))].show(5)

df.groupby(['age','y']).count()

#If we want to use different fields for sorting, or DESC instead of ASC
# we want to sort by our calculated field (count), this field needs to become part of 
# the DataFrame. After grouping , we get back a different type, called 
# a GroupByObject. 
# So we need to convert it back to a DataFrame. 
x = df.groupby(['marital', 'y']).count()
x.sort('marital','count',ascending=False).show()

#additionally filter grouped data using a HAVING condition. In Pandas, 
# you can use .filter() and provide a Python function (or a lambda) 
# that will return True if the group should be included into the result.

y = df[df.marital == 'married'].groupby('y').count()
y.show()
from pyspark.sql.functions import col
y.filter(col("count")>3000).show()

#creating new df
df1 = df.groupby(['age','y']).count()
-----------------------------------------
