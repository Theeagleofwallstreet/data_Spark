Download datasets repo from github
https://github.com/ajaykuma/Datasets

These datasets can be used to work with Spark(pyspark/spark-shell) or when
building your application using IDE
------------------------

$spark-shell --master yarn ( to start spark-shell with yarn)
[--master yarn is not needed if spark-defaults.conf already points to spark.master=yarn]
$spark-shell --master local (to start spark-shell in local mode)
$spark-shell --master spark://<hostname of spark master>:7077 (to start spark-shell if 
 running a spark standalone cluster)

sparkContext is the entry point of application
sparkSession is entry point of application which is tied to pyspark
The scala shell is REPL (Read Evaluate Print Loop)
Even though it appears as interpreter, all typed code is converted to Byte Code and executed)

scala> spark.<tab>
baseRelationToDataFrame   createDataFrame   experimental      range          sharedState    stop      udf       
catalog                   createDataset     implicits         read           sparkContext   streams   version   
close                     emptyDataFrame    listenerManager   readStream     sql            table               
conf                      emptyDataset      newSession        sessionState   sqlContext     time      

--val creates immutable structures
--var creates mutable structures 
-------
scala>
#val rdd = spark.sparkContext.parallelize(1 to 1000) Or
#val rdd = sc.parallelize( 1 to 1000)
#rdd.count()
#rdd.getNumPartitions
#rdd.size

--using filter transformation to filter out even numbers
#val result = rdd.filter(x => x%2 == 0) 
#result.collect()
#result.foreach(println)

--specifying partitions
#val rdd_spec_Partitions = sc.parallelize(1 to 1000,4)
#rdd.getNumPartitions

-------
--working with a file
Load a file in HDFS in 'mydata' folder
For example : cv999_14636.txt

--using method of SparkContext(sc) to create 1st RDD or entry into Spark
scala> val mydata = sc.textFile("/mydata/cv999_14636.txt")
mydata: org.apache.spark.rdd.RDD[String] = /mydata/cv999_14636.txt MapPartitionsRDD[3] at textFile at <console>:24

--map transformation
scala> val mydatauc = mydata.map(line => line.toUpperCase())
mydatauc: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at map at <console>:25

--filter transformation
scala> val mydataflt = mydatauc.filter(line => (line.startsWith("E")))
mydataflt: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:25

--invoking collect action
scala> mydataflt.collect
res0: Array[String] = Array("EMPHASIS ON THE WORD " PRESUMABLY . " ")           

--invoking take action and printing results
scala> mydataflt.take(2).foreach(println)
EMPHASIS ON THE WORD " PRESUMABLY . "                                           

--looking at lineage
scala> mydataflt.toDebugString
res2: String =
(2) MapPartitionsRDD[5] at filter at <console>:25 []
 |  MapPartitionsRDD[4] at map at <console>:25 []
 |  /mydata/cv999_14636.txt MapPartitionsRDD[3] at textFile at <console>:24 []
 |  /mydata/cv999_14636.txt HadoopRDD[2] at textFile at <console>:24 []

------
Chaining Transformations:

scala> val mydata = sc.textFile("/mydata/cv999_14636.txt").map(line => line.toUpperCase()).filter(line => (line.startsWith("E")))
mydata: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at filter at <console>:24
scala> mydata.collect
res3: Array[String] = Array("EMPHASIS ON THE WORD " PRESUMABLY . " ")

------
scala> val mydata = sc.textFile("/mydata/cv999_14636.txt")
mydata: org.apache.spark.rdd.RDD[String] = /mydata/cv999_14636.txt MapPartitionsRDD[11] at textFile at <console>:24

scala> val mydatauc = mydata.map(line => line.toUpperCase())
mydatauc: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at map at <console>:25

scala> val mydataflt = mydatauc.filter(line => (line.contains("AND")))
mydataflt: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at filter at <console>:25

scala> mydataflt.count
res6: Long = 8

scala> mydataflt.take(10).foreach(println)
-----
scala> mydata.getNumPartitions
res10: Int = 2

scala> mydatauc.getNumPartitions
res11: Int = 2

scala> mydataflt.getNumPartitions
res12: Int = 2

scala> mydataflt.saveAsTextFile("filt-result")

scala> mydataflt.repartition(1).saveAsTextFile("filt-result-new")

Looking in HDFS
auatltechnology@tempcluster-m:~$ hdfs dfs -ls -R /user/auatltechnology/filt-result
-rw-r--r--   2 auatltechnology hadoop          0 2021-10-01 13:18 /user/auatltechnology/filt-result/_SUCCESS
-rw-r--r--   2 auatltechnology hadoop       1115 2021-10-01 13:18 /user/auatltechnology/filt-result/part-00000
-rw-r--r--   2 auatltechnology hadoop        148 2021-10-01 13:18 /user/auatltechnology/filt-result/part-00001
auatltechnology@tempcluster-m:~$ hdfs dfs -ls -R /user/auatltechnology/filt-result-new
-rw-r--r--   2 auatltechnology hadoop          0 2021-10-01 13:18 /user/auatltechnology/filt-result-new/_SUCCESS
-rw-r--r--   2 auatltechnology hadoop       1263 2021-10-01 13:18 /user/auatltechnology/filt-result-new/part-00000
-----

Working with directory
--creates pair RDD with key and value(key being file name and path, value being file content)

scala> val mycont = sc.wholeTextFiles("/mydata")
mycont: org.apache.spark.rdd.RDD[(String, String)] = /mydata MapPartitionsRDD[1] at wholeTextFiles at <console>:24

scala> mycont.keys.take(10).foreach(println)
hdfs://tempcluster-m/mydata/cv999_14636.txt                                     

scala> mycont.values.take(1).foreach(println)
<content of file>

scala> mycont.keys.filter(n => n.contains("cv")).count
res2: Long = 1

scala> mycont.values.flatMap(n => n.split(" ")).filter(n => (n == "the")).count
res7: Long = 29

------

--Wordcount using RDD

scala> val doc = sc.textFile("/mydata/cv999_14636.txt")
doc: org.apache.spark.rdd.RDD[String] = /mydata/cv999_14636.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val words = doc.flatMap(n => n.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> val wordskv = words.map(n => (n,1))
wordskv: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:25

scala> val wordcount = wordskv.reduceByKey(_ + _)
wordcount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25

scala> wordcount.take(5).foreach(println)

#Additional processing
scala> val revrs = wordcount.map{case(word,i)=>i + "," + word}
revrs: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at map at <console>:25

scala> revrs.take(10).foreach(println)
scala> revrs.collect

scala> revrs.flatMap(n => n.split(" "))
res8: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at flatMap at <console>:26

scala> revrs.flatMap(n => n.split(" ")).take(2).foreach(println)
1,near
1,went

scala> revrs.flatMap(n => n.split(" ")).sortBy(n => n(1)).collect
scala> revrs.flatMap(n => n.split(" ")).sortBy(n => n(1),false).collect
------

--Creating Pair RDDs
--create a file 'sample1.txt'
hello this is a new file
hi this a a new cabinet
why is this file here
where is the new file
hi ,can we speak
hello welcome
hello this was a new subject
cool we are working
where is the new file

--create a second file 'sample2.txt'
hello this is a new book
hi this a a new cabinet
why is this file here
where is the new book
hi ,can we speak now
hello welcome here
hello this was a new subject
cool we are working & learning
where is the new file
this is a test
why is this a test

--push these files to 'cloud storage' or 'hdfs' or 'local machine'

scala> val data1 = sc.textFile("/mydata/sample1.txt")
data1: org.apache.spark.rdd.RDD[String] = /mydata/sample1.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val data2 = sc.textFile("/mydata/sample2.txt")
data2: org.apache.spark.rdd.RDD[String] = /mydata/sample2.txt MapPartitionsRDD[3] at textFile at <console>:24

scala> val pair1 = data1.map(x => (x.split(" ")(0),x))
pair1: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[4] at map at <console>:25

scala> val pair2 = data2.map(x => (x.split(" ")(0),x))
pair2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at <console>:25

scala> pair1.take(10).foreach(println)
(hello,hello this is a new file)                                                
(hi,hi this a a new cabinet)
...
(where,where is the new file)

scala> pair2.take(10).foreach(println)
(hello,hello this is a new book)
(hi,hi this a a new cabinet)
...
(where,where is the new file)
(this,this is a test)

scala> pair1.keys.take(10).foreach(println)
hello
hi
...
cool
where

scala> pair2.keys.take(10).foreach(println)
hello
hi
...
cool
where
this

scala> pair1.values.take(10).foreach(println)
hello this is a new file
hi this a a new cabinet
...
where is the new file

scala> pair2.values.take(10).foreach(println)
hello this is a new book
hi this a a new cabinet
...
where is the new file
this is a test

--Working on pairRDDs
scala> pair1.count
res8: Long = 9

scala> pair1.distinct.count
res9: Long = 8

scala> pair1.distinct.take(10).foreach(println)
(cool,cool we are working )
(hi,hi this a a new cabinet)
(hi,hi ,can we speak)
...
(where,where is the new file)

--wider transformations (which result in shuffling)
--sorting data by keys
scala> val sorted = pair1.sortByKey(true)
sorted: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[21] at sortByKey at <console>:25

scala> sorted.take(5).foreach(println)
(cool,cool we are working )
(hello,hello this is a new file)
(hello,hello welcome)
(hello,hello this was a new subject)
(hi,hi this a a new cabinet)

scala> val sorted = pair1.sortByKey(false)
sorted: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[24] at sortByKey at <console>:25

scala> sorted.take(5).foreach(println)
(why,why is this file here)
(where,where is the new file)
(where,where is the new file)
(hi,hi this a a new cabinet)
(hi,hi ,can we speak)

scala> val grouped = pair1.groupByKey()
grouped: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[25] at groupByKey at <console>:25

scala> grouped.take(5).foreach(println)
(hello,CompactBuffer(hello this is a new file, hello welcome, hello this was a new subject))
(why,CompactBuffer(why is this file here))
(cool,CompactBuffer(cool we are working ))
(hi,CompactBuffer(hi this a a new cabinet, hi ,can we speak))
(where,CompactBuffer(where is the new file, where is the new file))

scala> val counted = pair1.countByKey()
counted: scala.collection.Map[String,Long] = Map(why -> 1, cool -> 1, hi -> 2, where -> 2, hello -> 3)

scala> val counted = pair2.countByKey()
counted: scala.collection.Map[String,Long] = Map(this -> 1, why -> 2, cool -> 1, hi -> 2, where -> 2, hello -> 3)

--Working on multiple pair RDDs
scala> pair1.intersection(pair2).take(10).foreach(println)
(hi,hi this a a new cabinet)
(hello,hello this was a new subject)
(why,why is this file here)
(where,where is the new file)

scala> pair1.subtract(pair2).take(10).foreach(println)
(cool,cool we are working )
(hi,hi ,can we speak)
(hello,hello welcome)
(hello,hello this is a new file)

scala> pair1.union(pair2).take(10).foreach(println)
(hello,hello this is a new file)
(hi,hi this a a new cabinet)
...
(hello,hello this is a new book)

scala> pair1.join(pair2).take(10).foreach(println)
(hello,(hello this is a new file,hello this is a new book))
(hello,(hello this is a new file,hello welcome here))
...
(why,(why is this file here,why is this file here))

scala> pair1.leftOuterJoin(pair2).take(10).foreach(println)
(hello,(hello this is a new file,Some(hello this is a new book)))
(hello,(hello this is a new file,Some(hello welcome here)))
...
(why,(why is this file here,Some(why is this file here)))

scala> pair1.rightOuterJoin(pair2).take(10).foreach(println)
(this,(None,this is a test))
(hello,(Some(hello this is a new file),hello this is a new book))
...
(hello,(Some(hello this was a new subject),hello this was a new subject))

=========================

Using SparkSession (for structured data)
auatltechnology@tempcluster-m:~$ hdfs dfs -put /var/lib/hadoop-hdfs/Datasets-master/Bank_full.csv /mydata
auatltechnology@tempcluster-m:~$ hdfs dfs -put /var/lib/hadoop-hdfs/Datasets-master/recipes.json /mydata

--working with CSV
scala> val df1 = spark.read.load("/mydata/Bank_full.csv")
--throws error as format is not specified and default format expected in 'parquet'
[Caused by: java.lang.RuntimeException: hdfs://tempcluster-m/mydata/Bank_full.csv is not a Parquet file.]

scala> val df1 = spark.read.format("csv").load("/mydata/Bank_full.csv")
df1: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 16 more fields]

--schema is not inferred and headers are unknown
scala> df1.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

scala> df1.show(5)
+-----+---+------------+-------+---------+---------+-------+-------+----+-------+----+-----+--------+--------+-----+--------+--------+--
--+
|  _c0|_c1|         _c2|    _c3|      _c4|      _c5|    _c6|    _c7| _c8|    _c9|_c10| _c11|    _c12|    _c13| _c14|    _c15|    _c16|_c17|
+-----+---+------------+-------+---------+---------+-------+-------+----+-------+----+-----+--------+--------+-----+--------+--------+--
--+
|serNo|age|         job|marital|education|defaulter|balance|housing|loan|contact| day|month|duration|campaign|pdays|previous|poutcome| y|
|    1| 58|  management|married| tertiary|       no|   2143|    yes|  no|unknown|   5|  may|     261|       1|   -1|       0| unknown| no|

scala> df1.select($"serNo",$"age").show(5)
+-----+---+
|serNo|age|
+-----+---+
|    1| 58|
|    2| 44|
|    3| 33|
|    4| 47|
|    5| 33|
+-----+---+
only showing top 5 rows

scala> df1.select($"serNo",$"age").filter($"age" > 75).show(5)
+-----+---+
|serNo|age|
+-----+---+
|29159| 83|
|29323| 83|
|30909| 76|
|31052| 83|
|31056| 85|
+-----+---+
only showing top 5 rows

--writing df to storage
scala> df1.write.format("csv").save("/dfwrites/bankdata")
scala> df1.write.format("csv").save("/dfwrites/bankdata_csv")

scala> df1.write.saveAsTable("bankdata_tbl")
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used

--working with JSON
scala> val recp = spark.read.json("/mydata/recipes.json")
recp: org.apache.spark.sql.DataFrame = [cookTime: string, datePublished: string ... 7 more fields]
scala> recp.printSchema()
root
 |-- cookTime: string (nullable = true)
 |-- datePublished: string (nullable = true)
 |-- description: string (nullable = true)
 |-- image: string (nullable = true)
 |-- ingredients: string (nullable = true)
 |-- name: string (nullable = true)
 |-- prepTime: string (nullable = true)
 |-- recipeYield: string (nullable = true)
 |-- url: string (nullable = true)

continued...
